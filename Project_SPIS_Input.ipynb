{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varun-626/flask-hello-world/blob/master/Project_SPIS_Input.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPFvm2inabfS",
        "outputId": "f5082406-df71-49c4-bf71-185d06bfdc1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to filteredTweets.csv\n"
          ]
        }
      ],
      "source": [
        "'''This code cleans up the tweets_01-08-2021.csv file and saves the 'text', 'retweets' & 'date' columes to a new .csv file'''\n",
        "\n",
        "# 'text', 'isRetweet', 'isDeleted', 'favorites', 'retweets', 'date', 'device'\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def remove_https_links_from_csv(input_filepath, output_filepath):\n",
        "        with open(input_filepath, 'r') as infile, open(output_filepath, 'w') as outfile:\n",
        "            for line in infile:\n",
        "                # Use regex to find and replace HTTPS links\n",
        "                # This regex matches http or https followed by anything until a space or end of line\n",
        "                cleaned_line = re.sub(r'https?://\\S+', '', line)\n",
        "                outfile.write(cleaned_line)\n",
        "\n",
        "# Load the original CSV\n",
        "df = pd.read_csv('tweets_01-08-2021.csv')\n",
        "\n",
        "# Remove rows where the 'device' column is 'TweetDeck'\n",
        "df_filtered = df[df['device'] != 'TweetDeck']\n",
        "\n",
        "# Remove rows where the 'isRetweet' column is 't' (true)\n",
        "df_filtered = df[df['isRetweet'] != 't']\n",
        "\n",
        "# Remove rows where the 'isDeleted' column is 't' (true)\n",
        "df_filtered = df[df['isDeleted'] != 't']\n",
        "\n",
        "# Drop unwanted columns\n",
        "columns_to_remove = ['id', 'device', 'favorites', 'isFlagged', 'isDeleted', 'isRetweet']\n",
        "df_new = df.drop(columns=columns_to_remove)\n",
        "\n",
        "# Words to remove\n",
        "words_to_remove = ['RT']\n",
        "\n",
        "# Remove words from 'text' column\n",
        "df['text'] = df['text'].replace(\n",
        "    to_replace=r'\\b(?:' + '|'.join(words_to_remove) + r')\\b',\n",
        "    value='',\n",
        "    regex=True\n",
        ")\n",
        "\n",
        "# Regex pattern to match words starting with @ or # or http(s)\n",
        "pattern_at = r'\\s*@\\w+'\n",
        "pattern_hash = r'\\s*#\\w+'\n",
        "pattern_https = r'https:\\/\\/\\S+'\n",
        "\n",
        "# Apply the pattern to the column\n",
        "df['text'] = df['text'].str.replace(pattern_at, '', regex=True).str.strip()\n",
        "df['text'] = df['text'].str.replace(pattern_hash, '', regex=True).str.strip()\n",
        "df['text'] = df['text'].str.replace(pattern_https, '', regex=True).str.strip()\n",
        "\n",
        "# Strip extra whitespace\n",
        "df['text'] = df['text'].str.strip()\n",
        "\n",
        "# Drop rows where 'text' column is empty\n",
        "df = df.dropna(subset=['text'])\n",
        "\n",
        "\n",
        "remove_https_links_from_csv('tweets_01-08-2021.csv', 'filteredTweets.csv')\n",
        "\n",
        "# Save to a new CSV file\n",
        "df.to_csv('filteredTweets.csv', index=False)\n",
        "\n",
        "print(\"Saved to filteredTweets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''This code takes the filteredTweets.csv file and saves it as trumpTweets.txt in an easy to read format'''\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('filteredTweets.csv')\n",
        "\n",
        "# Drop rows with missing values in either column\n",
        "df = df.dropna(subset=['text', 'date'])\n",
        "\n",
        "# Convert everything to string (safe for numbers)\n",
        "df['text'] = df['text'].apply(str)\n",
        "\n",
        "#df['date'] = df['date'].apply(str)\n",
        "\n",
        "# Combine columns (\"text: date\")\n",
        "combined = df['text'] #+ ': ' + df['date']\n",
        "\n",
        "# Combine into one text block (each tweet on its own line)\n",
        "text = \"\\n\".join(combined)\n",
        "\n",
        "# Save to TXT file\n",
        "with open('trumpTweets.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(text)\n",
        "\n",
        "# Read the trumpTweets.txt file\n",
        "with open('trumpTweets.txt', 'r', encoding='utf-8') as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "# Filter out empty or whitespace-only lines\n",
        "cleaned_lines = [line for line in lines if line.strip() != '']\n",
        "\n",
        "# Overwrite the original\n",
        "with open('trumpTweets.txt', 'w', encoding='utf-8') as outfile:\n",
        "    outfile.writelines(cleaned_lines)\n",
        "\n",
        "print(\"Saved to trumpTweets.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhivkQSuxNiB",
        "outputId": "d441fdd8-1281-40bd-ad23-17df64070ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to trumpTweets.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import csv\n",
        "import re\n",
        "\n",
        "t = []\n",
        "\n",
        "with open('tweets_01-08-2021.csv', 'r') as csv_file:\n",
        "\n",
        "  csv_reader = csv.DictReader(csv_file)\n",
        "  for row in csv_reader:\n",
        "    match = re.search(r'\\b\\d{2}:\\d{2}:\\d{2}\\b', row['date'])\n",
        "    time_string = match.group() if match else ''\n",
        "    t.append(time_string)\n",
        "\n",
        "def formatdate(date):\n",
        "\n",
        "    dateFormat = '%Y-%m-%d %H:%M:%S'\n",
        "    d = datetime.strptime(date, dateFormat)\n",
        "    return d.strftime('%m/%d/%Y')\n",
        "\n",
        "listTweet = []\n",
        "USDDollarIndex = []\n",
        "SPDollarIndex = []\n",
        "\n",
        "def datesort(date):\n",
        "  data = date\n",
        "\n",
        "  sorted_data_desc = sorted(data, key=lambda item: datetime.strptime(item[0], \"%m/%d/%Y\"), reverse=True)\n",
        "\n",
        "\n",
        "DollarIndexChangeFormatted = []\n",
        "\n",
        "output_file = 'DataInputwTimeStamp.csv'\n",
        "\n",
        "with open('tweets_01-08-2021.csv', 'r', newline='') as filteredTweets_csv:\n",
        "    reader = csv.DictReader(filteredTweets_csv)\n",
        "\n",
        "    for row in reader:\n",
        "        textValue = row['text']\n",
        "        dateValue = row['date']\n",
        "        dateValue = formatdate(dateValue)\n",
        "        listTweet.append([dateValue, textValue])\n",
        "\n",
        "with open('US Dollar Index Historical Data.csv', 'r', newline='') as USDIHD_csv:\n",
        "    reader = csv.DictReader(USDIHD_csv)\n",
        "\n",
        "    #print(reader.fieldnames) for debugging\n",
        "\n",
        "    for row in reader:\n",
        "        changeValue = row['Change %']\n",
        "        dateValue = row['\\ufeff\"Date\"']\n",
        "        USDDollarIndex.append([dateValue, changeValue])\n",
        "\n",
        "with open('S&P 500 Historical Data (1).csv', 'r', newline='') as SP_csv:\n",
        "    reader = csv.DictReader(SP_csv)\n",
        "\n",
        "    print(reader.fieldnames)\n",
        "\n",
        "    for row in reader:\n",
        "        changeValue = row['Change %']\n",
        "        dateValue = row['\\ufeff\"Date\"']\n",
        "        SPDollarIndex.append([dateValue, changeValue])\n",
        "\n",
        "# Convert into a dictionary for faster lookup\n",
        "dictDI = {row[0]: row[1:] for row in USDDollarIndex}\n",
        "dictSP = {row[0]: row[1:] for row in SPDollarIndex}\n",
        "\n",
        "# Combine based on matching dates\n",
        "combined = []\n",
        "\n",
        "for row in listTweet:\n",
        "    date = row[0]\n",
        "    if date in dictDI and date in dictSP:\n",
        "        combined.append([date]+ row[1:] + dictDI[date] + dictSP[date])\n",
        "\n",
        "saver = []\n",
        "for x in range(len(combined)):\n",
        "    combined[x][2] = combined[x][2].replace('%', '')\n",
        "    combined[x][2] = float(combined[x][2])\n",
        "    combined[x][3] = combined[x][3].replace('%', '')\n",
        "    combined[x][3] = float(combined[x][3])\n",
        "    combined[x].append(t[x])\n",
        "    if len(combined[x][1]) < 5:\n",
        "      saver.append(x)\n",
        "\n",
        "for x in saver:\n",
        "  try :\n",
        "      combined.remove(combined[x])\n",
        "  except:\n",
        "      pass\n",
        "\n",
        "#datesort(combined)\n",
        "#print(combined)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(combined, columns=['Date', 'Tweet', 'USDChange%', 'SPChange%', 'Timestamp'])\n",
        "\n",
        "df['Tweet'].replace('', np.nan, inplace=True)\n",
        "df['Tweet'].replace(':', np.nan, inplace=True)\n",
        "\n",
        "df_cleaned = df.dropna()\n",
        "\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "file_path = output_file\n",
        "column_to_check = 'Tweet'\n",
        "\n",
        "with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    rows = [row for row in reader if row[column_to_check] and row[column_to_check].strip()]\n",
        "    fieldnames = reader.fieldnames\n",
        "\n",
        "with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)\n",
        "\n",
        "df = pd.read_csv('DataInputwTimeStamp.csv')\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "sorted_df = df.sort_values(by='Date', ascending=True)\n",
        "sorted_df.to_csv('DataInputwTimeStamp.csv', index=False)"
      ],
      "metadata": {
        "id": "cFMApQ0KehAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9efb63-a2d0-4984-8bab-58506cb664b4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeff\"Date\"', 'Price', 'Open', 'High', 'Low', 'Vol.', 'Change %']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1327919553.py:100: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Tweet'].replace('', np.nan, inplace=True)\n",
            "/tmp/ipython-input-1327919553.py:101: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Tweet'].replace(':', np.nan, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('S&P 500 Historical Data (1).csv', 'r', newline='') as SP_csv:\n",
        "    reader = csv.DictReader(SP_csv)\n",
        "\n",
        "    print(reader.fieldnames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwFZtOAIU8yp",
        "outputId": "accea867-748e-4042-940b-3abb9d5478a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeff\"Date\"', 'Price', 'Open', 'High', 'Low', 'Vol.', 'Change %']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "path_to_file = 'trumpTweets.txt'\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Split text charecters into tokens before conversion\n",
        "chars = tf.strings.unicode_split(text, input_encoding='UTF-8')\n",
        "\n",
        "# Convert tokens into nmeric IDs\n",
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "\n",
        "# To reverse\n",
        "\n",
        "def text_from_ids(ids):\n",
        "  chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "print(text_from_ids(ids).numpy())\n",
        "\n",
        "#Convert text vector into a stream of charecter indices\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "print(all_ids)\n",
        "\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "seq_length = 100\n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "#for seq in sequences.take(1):\n",
        "#print(chars_from_ids(seq))\n",
        "\n",
        "#joining tokens into string\n",
        "\n",
        "#for seq in sequences.take(5):\n",
        "#  print(text_from_ids(seq).numpy())\n",
        "\n",
        "\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "XHhqF0CmVNRK",
        "outputId": "a0df5258-57e3-4ce4-c692-836ffc0b2832",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nimport tensorflow as tf\\n\\nimport numpy as np\\nimport os\\nimport time\\n\\npath_to_file = \\'trumpTweets.txt\\'\\n\\n# Read, then decode for py2 compat.\\ntext = open(path_to_file, \\'rb\\').read().decode(encoding=\\'utf-8\\')\\n\\n# Split text charecters into tokens before conversion\\nchars = tf.strings.unicode_split(text, input_encoding=\\'UTF-8\\')\\n\\n# Convert tokens into nmeric IDs\\nids_from_chars = tf.keras.layers.StringLookup(\\n    vocabulary=list(vocab), mask_token=None)\\n\\nids = ids_from_chars(chars)\\n\\n# To reverse\\n\\ndef text_from_ids(ids):\\n  chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\\n  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\\nprint(text_from_ids(ids).numpy())\\n\\n#Convert text vector into a stream of charecter indices\\nall_ids = ids_from_chars(tf.strings.unicode_split(text, \\'UTF-8\\'))\\nprint(all_ids)\\n\\nids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\\n\\nfor ids in ids_dataset.take(10):\\n    print(chars_from_ids(ids).numpy().decode(\\'utf-8\\'))\\n\\nseq_length = 100\\n\\nsequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\\n\\n#for seq in sequences.take(1):\\n#print(chars_from_ids(seq))\\n\\n#joining tokens into string\\n\\n#for seq in sequences.take(5):\\n#  print(text_from_ids(seq).numpy())\\n\\n\\ndef split_input_target(sequence):\\n    input_text = sequence[:-1]\\n    target_text = sequence[1:]\\n    return input_text, target_text\\n\\nsplit_input_target(list(\"Tensorflow\"))\\ndataset = sequences.map(split_input_target)\\n\\nfor input_example, target_example in dataset.take(1):\\n    print(\"Input :\", text_from_ids(input_example).numpy())\\n    print(\"Target:\", text_from_ids(target_example).numpy())\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXSwHPpDM4uWX74ujy4mdP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}